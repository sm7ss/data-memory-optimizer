# üöÄ Data Memory Optimizer

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Polars](https://img.shields.io/badge/Polars-Fast__DataFrames-red.svg)](https://pola.rs/)
[![PyArrow](https://img.shields.io/badge/PyArrow-Efficient__Memory-orange.svg)](https://arrow.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Una herramienta inteligente de optimizaci√≥n de memoria que analiza autom√°ticamente archivos de datos (CSV/Parquet) y determina la estrategia √≥ptima de procesamiento (`eager`, `lazy`, o `streaming`) basada en los recursos del sistema.

## üîç El Problema que Resuelvo

Como Data Engineer, constantemente enfrento la pregunta: **¬øC√≥mo procesar este archivo sin saturar la memoria del sistema?** 

Esta herramienta responde autom√°ticamente esa pregunta analizando:
- üìä **Overhead espec√≠fico** de CSV vs Parquet
- üíæ **Recursos disponibles** del sistema
- üìè **Tama√±o y estructura** de los datos
- ‚ö° **Estrategia √≥ptima**: eager, lazy, o streaming

## üéØ Caracter√≠sticas Principales

### üìà **An√°lisis Inteligente de Archivos**

- Estimaci√≥n precisa de overhead por tipo de columna
- An√°lisis de strings y su impacto en memoria
- Soporte para CSV y Parquet con algoritmos especializados

### üí° **Decision Making Autom√°tico**

- Basado en ratio memoria estimada / memoria disponible
- Umbrales optimizados por experiencia emp√≠rica
- Considera margen de seguridad para el sistema operativo

### üìä **M√©tricas y Resultados Medibles**

- Ratio de utilizaci√≥n de memoria
- Overhead estimado por tipo de dato
- Recomendaci√≥n cuantificada y justificada

## üöÄ Instalaci√≥n y Uso R√°pido

### **Instalaci√≥n**

```bash
git clone https://github.com/sm7ss/data-memory-optimizer.git
cd data-memory-optimizer
pip install -r requirements.txt
```

### **Uso B√°sico**

```python 
from src.path_desicion_maker import PipelineEstimatedSizeFiles

# Analizar un archivo
analyzer = PipelineEstimatedSizeFiles("datos.csv")
result = analyzer.estimated_size_file()

print(f"Estrategia recomendada: {result['decision']}")
print(f"Ratio memoria: {result['ratio']}")
print(f"Memoria estimada: {result['memoria_total_estimada_gb']} GB")
```

## üìä Resultados de Pruebas (M√©tricas Reales)

### üìà **Archivo Parquet Grande**

```python 
{
    'ratio': 1.325,
    'overhead_estimado': 1.36,
    'safety_memory': 4.667,
    'archivo_descomprimido_gb': 5.612,
    'total_de_filas_gb': 244673551,
    'memoria_total_estimada_gb': 7.632,
    'memoria_disponible': 10.426,
    'total_memory': 15.555,
    'decision': 'lazy'  # ‚úÖ Recomendaci√≥n √≥ptima
}
```

### üìä **Archivo CSV Peque√±o**

```python 
{
    'ratio': 0.0,
    'total_rows': 89,
    'bytes_por_columna': 148.0,
    'safety_memory': 4.667,
    'memoria_total_estimada_gb': 0.0,
    'memoria_disponible': 10.424,
    'total_memory': 15.555,
    'decision': 'eager'  # ‚úÖ Puede cargarse completo
}
```

## üîß Algoritmos Implementados

### **CSV Overhead Analysis**

```python 
class CsvOverhead:
    def string_csv_overhead(self) -> float:
        # An√°lisis de overhead espec√≠fico de strings en CSV
        avg_len = sum(self.frame_sample[col].str.len_bytes().median() 
                     for col in self.str_columns) / len(self.str_columns)
        
        # Asignaci√≥n de factor basado en longitud promedio
        if avg_len <= 1: return 2.0
        elif avg_len <= 5: return 1.8
        elif avg_len <= 10: return 1.6
        # ... l√≥gica optimizada
```

### **Parquet Overhead Analysis**

```python 
class ParquetOverheadEstimator:
    def string_overhead(self) -> float:
        # C√°lculo especializado para strings en Parquet
        avg_string_len = sum([
            sample_median[col].str.len_bytes().sum() 
            for col in string_columns
        ]) / len(string_columns)
        
        base = 1.0 + 4.0 / avg_string_len
        # Ajustes basados en benchmarks reales
```

### **Decision Making Engine**

```python 
class FileSizeEstimator:
    def estimate_csv_size(self, csv_overhead_class) -> Dict[str, Any]:
        # C√°lculo de ratio cr√≠tico
        ratio = estimated_memory / usable_ram
        
        # L√≥gica de decisi√≥n optimizada
        if ratio <= 0.65: return 'eager'
        elif ratio <= 2.0: return 'lazy'
        else: return 'streaming'
```

## üéØ L√≥gica de Decisi√≥n

### üìà **Thresholds Optimizados**

| Ratio (Estimado/Disponible) | Estrategia | Justificaci√≥n                          |
|-----------------------------|------------|----------------------------------------|
| **‚â§ 0.65**                  | eager      | Memoria suficiente para carga completa |
| **0.65 - 2.0**              | lazy	   | Procesamiento por lotes recomendado    |
| **> 2.0**	                  | streaming  | Requiere procesamiento incremental     |

### üõ°Ô∏è **Safety Margins**

- 30% de memoria reservada para sistema operativo
- An√°lisis por tipo de dato con overhead espec√≠fico
- Consideraci√≥n de strings como mayor impacto

### üì¶ **Dependencias**

polars>=0.19.0
pyarrow>=12.0.0
psutil>=5.9.0

## üèÜ Casos de Uso en Producci√≥n

### **1. Pipeline de ETL Automatizado**

```python 
# Integraci√≥n en pipeline de data engineering
def process_file_optimally(file_path: str):
    analyzer = PipelineEstimatedSizeFiles(file_path)
    result = analyzer.estimated_size_file()
    
    if result['decision'] == 'eager':
        return pl.read_csv(file_path)  # Carga completa
    elif result['decision'] == 'lazy':
        return pl.scan_csv(file_path)  # Procesamiento lazy
    else:
        return pl.scan_csv(file_path).collect(streaming=True)  # Streaming
```

### **2. Sistema de Monitoreo de Recursos**

```python 
# Monitoreo proactivo de uso de memoria
class ResourceMonitor:
    def check_file_safety(self, file_path: str):
        result = PipelineEstimatedSizeFiles(file_path).estimated_size_file()
        if result['ratio'] > 1.5:
            logger.warning(f"Archivo {file_path} puede saturar memoria")
            return False
        return True
```

### **3. Optimizaci√≥n de Queries**

```python 
# Selecci√≥n autom√°tica de estrategia de query
def optimize_query(file_path: str, query):
    result = PipelineEstimatedSizeFiles(file_path).estimated_size_file()
    
    if result['decision'] == 'streaming':
        return execute_streaming_query(file_path, query)
    else:
        return execute_standard_query(file_path, query)
```

## üî¨ Detalles T√©cnicos Avanzados

### **Overhead por Tipo de Dato**

| Tipo de Dato	    | CSV Overhead | Parquet Overhead | Justificaci√≥n        |
|-------------------|--------------|------------------|----------------------|
| **Int8/16/32/64** | 1.4-1.55     | 1.2-1.35         |	Overhead de encoding |
| **Float32/64**	| 1.6-1.65	   | 1.4-1.45         |	Precisi√≥n decimal    |
| **String**	    | 1.1-2.0	   | 1.05-2.2         |	Longitud variable    |
| **Boolean**	    | 2.5	       | 2.0	          | Ineficiente en texto |
| **DateTime**	    | 1.85	       | 1.55	          | Formato timestamp    |

### **F√≥rmulas de Estimaci√≥n**

```bash
# Memoria Estimada CSV
memoria_csv = (filas √ó overhead_promedio √ó bytes_por_columna) / 1024¬≥

# Memoria Estimada Parquet  
memoria_parquet = (overhead_promedio √ó tama√±o_descomprimido) / 1024¬≥

# Ratio de Decisi√≥n
ratio = memoria_estimada / (memoria_disponible - margen_seguridad)
```

## ü§ù Contribuci√≥n

¬°Contribuciones son bienvenidas! Si tienes ideas para mejorar el algoritmo o agregar nuevas caracter√≠sticas:

1. Haz fork del proyecto
2. Crea una rama para tu feature (git checkout -b feature/mejora-algoritmo)
3. Commit tus cambios (git commit -m 'Agregar soporte para formato X')
4. Push a la rama (git push origin feature/mejora-algoritmo)
5. Abre un Pull Request

## üë©‚Äçüíª Sobre Este Proyecto

Este proyecto naci√≥ de la necesidad pr√°ctica de o**ptimizar el uso de memoria en pipelines de data engineering**. Como Data Engineer, constantemente enfrentaba el dilema de elegir entre eager, lazy y streaming sin m√©tricas concretas.

Las **m√©tricas presentadas son reales** y representan pruebas con archivos de diferentes tama√±os y complejidades. Cada decisi√≥n est√° respaldada por an√°lisis cuantitativo y validaci√≥n emp√≠rica.

**¬øPreguntas o sugerencias?** ¬°No dudes en abrir un issue!
